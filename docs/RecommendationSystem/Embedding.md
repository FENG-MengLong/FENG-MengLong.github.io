## 什么是embedding？
Embedding是一种稠密向量的表示形式，在 embedding 大行其道之前，机器学习领域最受大家欢迎的是一个叫做one hot 表示的

one hot是一种用01编码文字的方式，维度等于样本数量（比如字符库里有10个字，维度就有10，某个字对应的位置为1，其余位为0），是很高维的一种表示方法，太占内存空间

#### Embedding在神经网络中是什么意思？

一般意义的 embedding 则是神经网络的参数权重，只具有整体意义和相对意义，没有绝对物理意义。靠近底层的更加基本和普适（e.g.线条），靠近上层的更贴近训练模型（E.G.人脸），所以往往先用pretrain做底层训练，再用fine-tuning做适合自己模型的高层训练

这与 embedding 的产生过程有关，任何 embedding 一开始都是一个随机数，然后随着优化算法，不断迭代更新，最后网络收敛停止迭代的时候，网络各个层的参数就相对固化， 得到隐层权重表（这就是embedding），然后在通过查表可以单独查看每个元素的 embedding。

## Embedding的优势
#### 降维，稠密
embedding 相当于对高维数据用类似矩阵乘法的方式进行降维处理，避免维度灾难
直观上看 embedding 相当于是对 one Hot 表示做了平滑，而 one Hot 相当于是对 embedding 做了 max pooling

#### 词向量
早期用字符串来表示词，更高维稀疏，难以涵盖所有语料，而且无法表示语义，两个意思相近的词也会差之千里

通过计算向量之间的夹角来评估词向量的语义相关性，突破“精确匹配”的信息茧房问题


## 词嵌入（word embedding）（作为input）
自然语言中最小单位是词，一个词拥有词义、词性等属性，将这些属性表示成数值的形式，嵌入到数学空间，称为词嵌入（word embedding）

为啥做词嵌入呢？机器学习中针对一系列样本(x,y)，这里 x 是词语，y 是词性，
我们要构建 f(x)->y 的映射，而这里的数学模型 f（比如神经网络、SVM），只接受数值型输入

Word2vec是词嵌入的一种，在这里(x,y)是一个词和它的上下文，f是语言模型（language model），用于判断x和y放在一起是不是人话。但我们最终需要的是模型中各层神经网络的权重，即embedding

词向量：就是词嵌入，它包含一个词各种属性（词义、词性等），向量间的距离表示其相关性

## 语言模型f(x,y)

语言模型指的是“CBOW”和“Skip-gram”模型，两种模型结构如下，都是在基本网络结构上省去了hidden layer。

隐藏层的意义就是把输入数据的特征，抽象到另一个维度空间，来展现其更抽象化的特征，这些特征能更好的进行线性划分。是为方便计算的，不是必须项

投影层projection layer：做线性仿射以便于存储，可以减少存储量

具体学习过程会用到两个降低复杂度的近似方法——Hierarchical Softmax
或Negative Sampling

#### CBOW（Continuous Bag-of-Words Model）
是一种根据上下文的词语预测当前词语的出现概率的模型。已知上下文，在语料库中寻找概率最大的词汇，即最大似然函数的参数学习问题。也可看作一个多分类问题，即在众多选项中挑一个。

经典方法：softmax回归，对所有可能的参数进行遍历计算，得到一个概率最大的，当样本库庞大时复杂度过高

近似方法：Hierarchical Softmax，基于霍夫曼树。

使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为V,现在变成了log2V。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。

流程是？构建霍夫曼树，从根节点开始每个延申都是一次逻辑回归，这是一个概率判别模型，用sigmoid激活函数来做判别，最大化似然函数

#### Skip-gram
逆转了CBOW的因果关系而已，即已知当前词语，预测上下文
skip-gram更加低频友好，CBOW更加高频友好？

## embedding在推荐系统的应用

从博闻强记的传统LR到泛化能力更好的embedding

**LR（强记忆算法）→LR+特征工程（细化粒度，使之可扩展）→FM/FFM（隐向量二阶交叉）→embedding（深度学习，多维交叉，泛化能力）**

逻辑回归（LR）
逻辑回归是一个概率判别模型，属于线性分类中的软输出模型，也即输出值是[0，1]之间的概率，可以认为是一个打分器，对不同的特征或特征组合进行打分。得分越高也就是概率越高。最终给一个<user, context, item>的打分是其命中的打分器中所有条目的得分总和。

LR强于记忆，弱于扩展。LR的特点就是强于记忆，只要打分器足够大（比如几千亿项），它能够记住历史上的发生过的所有模式（i.e., 特征及其组合），所有的模式都依赖人工输入。LR本身并不能够发掘出新模式，它只负责评估各模式的重要性。（通过Cross Entropy Loss + SGD学习得到）

LR不发掘新模式，反之它能够通过regularization，能够剔除一些罕见模式（比如<中国人，于谦在非洲吃的同款恩希玛>），避免过拟合的同时降低规模。所以更适用于大众需求，无法满足偏小众的个性化需求，这导致各家企业的竞争主要在于数据量的竞争，数据量大，对大众的学习更准确。

通过人工做特征工程，将小众的需求拆解为概念的特征向量，能够细化粒度，使LR拥有扩展能力，然而也有缺点：①工作量大；②人工拆解有局限性，可能会有角度的疏漏
深度学习的embedding层

深度学习对于推荐算法的贡献与提升，其核心就在于Embedding。
Embedding是一门自动将概念拆解为特征向量的技术，目标是提升推荐算法的扩展能力，
从而能够自动挖掘那些低频、长尾、小众的模式，拥抱个性化推荐。

同时，embedding能够将意思相近的词汇聚起来，用户搜索标签时，会在标签向量的周围做top-K近邻搜索，以打破精确匹配带来的信息茧房。


wide & deep，就是把LR的记忆性+神经网络的泛化性进行了一个综合的适配

## 答疑补充

**提问：梯度消失问题**

梯度消失是神经网络里面一个非常可怕的问题，梯度消失后，会导致我们整个网络的训练没有任何的意义。因为更新我们的参数，需要通过梯度来进行更新，而梯度提前变成了0，会导致我们的整个迭代没有非常的充分，就提前终止了

针对梯度消失的问题，tanx, softmax,和ReLU等是通过使用不同的激活函数，使神经网络的表达能力进一步增强。还有LSTM等基于模型优化的方法，但其都只能缓解梯度消失的问题，不能根本上解决


**提问：为什么skip-gram更加低频友好，CBOW更加高频友好？**

**提问：embedding的推广**

uid embedding、item embedding、graph embedding等。
对应文本中多个词汇的共现是有规律的，在其他场景中用户行为序列也是有规律的，我们可以收集用户行为序列进行embedding分析

**提问：gnn 的embedding的计算方法，除去 skip-gram or cbow还有啥？**

skip-gram based的方法，deep walk，我们生成序列的时候，采用随机游走的方式， airbnb 在2018年的kdd发表过一篇文章，airbnb embedding针对skip-gram做了一些改动， 要引入hard 信息，如果某些内容不点击，或者明显的不喜欢，会更充分一些（类似加入罚数）

line算法， 阿里巴巴提出的一个算法，平时的构建图的时候，考虑的是一阶（彼此相连），但是很多时候我们的图上是没有那么多的一阶信息的，两个点有可能更相似，所以我们也可以引入二阶信息，中间有一个邻居，这也是一种去计算图的embedding的方法

eges，一些外部的side information 辅助决策graph convoluational 方法，来进行训练

**提问：fintune和增量学习，形式上差异在哪？都是新加入样本，调参**

fine tune其实就是在一个初始值的基础上，使得我们的网络能够在一个比较好的初始值上进行训练，fine tune之前，我们的这个pretrain好的东西具有
适配性，不仅是我们这个任务，其他任务也可以共享，fine tune就是我们在预训练的基础上，需要适配自己的任务，让对应的参数朝着自己的目标任务
，进行求解的过程，更具有一定的适配性

增量学习就是在解决模型训练过程中，以BP算法为基础的深度学习算法在新任务训练
的时候，在老任务上的表现会下降（遗忘灾难）。

出现遗忘灾难的原因在于传统算法假定数据分布是固定的，训练样本是iid，当变为连续流数据时，训练数据的分布就是非平稳的，模型从非平稳的数据分布中持续不断地获取知识时，新知识会干扰旧知识，从而导致模型性能的快速下降，甚至完全覆盖或遗忘以前学习到的旧知识

如何解决：最笨的方法，用所有已知的数据重新训练网络参数，然而效率太低，无法实时学习；可以用正则化方法，通过给新任务的损失函数施加约束的方法来保护旧知识不被新知识覆盖，优点是无需重复学习，相关算法有LwF算法（Learning without Forgetting (ECCV 2016)）；也有回放的方法，思想是“温故而知新”，要保留旧任务的部分数据，利用旧数据与新数据一起训练模型

**提问：在推荐中word2vec的序列长度一般怎么选，怎么获取长短期兴趣？**
平衡长短期兴趣，其实就是引入时间的衰减因子，离当前时间近的，尽量保留，远的开始衰减，借助牛顿冷却定理；
在实际工程应用中，序列长度本就是各异的，无所谓，针对序列长度一般的做法是先干掉作弊的流量，再以session划分，例如“10分钟以内，所有点击过的内容信息”可作为一个session

**提问：对于损失函数，为什么分类问题用交叉熵，回归问题用MSE？**
分类问题是要比较类别之间的概率大小，选择概率高的结果，描述两分布概率差异的可以用交叉熵；而回归问题，例如最小二乘法等，需要做到方差最小，以满足回归精确度，所以用MSE



